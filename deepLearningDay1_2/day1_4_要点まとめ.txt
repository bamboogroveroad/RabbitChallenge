勾配降下法について
・勾配降下法はNNを学習させる手法のことをいう。
・深層学習の目的は、学習を通して誤差を最小にするネットワークを作成すること
⇒誤差E(w)を最小化するパラメータwを発見すること。
⇒勾配降下法を利用してパラメータを最適化（一番傾きが0に近いところが目標）
⇒勾配降下法でのパラメータの更新方法、数式と、ソースコードについて
・学習率について。学習率が大きいと発散してしまう、
・学習率小さすぎると収束に時間がかかってしまう。また、少し下がったところ（局所極小解）で学習が終わってしまう。
⇒学習率の値によって学習効率が異なる。
・学習率の決定、収束性向上のためのアルゴリズムがいくつか公開され、利用されている。（Adam等）
・誤差関数の値をより小さくする方向に重みw、バイアスbを更新し次の周(エポック)に反映、数100エポック位回すとある程度結果が出る。

確率的勾配降下法（SGD）について
勾配降下法が全サンプルの平均誤差を利用するのに対して、ランダムに抽出した（毎回異なる）サンプルの誤差を利用する。
⇒局所極小解に収束するリスクが軽減、計算コストの軽減、オンライン学習ができる（通常の勾配降下法はバッチ学習）というメリット

ミニバッチ勾配降下法について
・オンライン学習の特徴をうまくバッチ学習に取り入れた手法
ランダムに分割したデータの集合（ミニバッチ）Dtに属するサンプルの平均誤差を利用する。
⇒確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用できる
→CPUを利用したスレッド並列化やGPUを利用したSingleInstructionM]ultiData（SIMD）並列化
