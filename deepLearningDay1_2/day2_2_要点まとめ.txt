・学習率最適化(optimizer)手法について
・NNを適切に学習するためには、（勾配降下法で利用する）学習率をどのようにしたらよいのか。
・学習率が大きすぎると発散する、逆に小さすぎると収束に時間がかかったり大域的最適値に収束しづらくなる。
初期の学習率設定方法の指針
・初期の学習率を大きく設定、徐々に学習率を小さくしていく。
・パラメータ毎に学習率を可変させる。
→学習率最適化手法を利用して学習率を最適化する。
・以下、4手法を説明する。

〇モメンタム
勾配降下法では、誤差をパラメータ（重み）で微分したものと学習率の積を減算するが、
モメンタムでは、誤差をパラメータ（重み）で微分したものと学習率の積を減算した後、
現在の重みに、前回の重みを減算した値と慣性の積を加算する。

モメンタムのメリット
・局所最適解にはならず、大域的最適解となる。
・谷間についてから最も低い位置（最適値）にいくまでの時間が早い。（移動平均のイメージ）

〇AdaGrad
勾配降下法では、誤差をパラメータ（重み）で微分したものと学習率の積を減算するが、
AdaGradでは、誤差をパラメータ（重み）で微分したものと再定義した学習率の積を減算する。

AdaGradのメリット
・勾配の穏やかな斜面に対して、最適値に近づける
AdaGradの課題
・学習率が徐々に小さくなるので、鞍点問題が起きる。

→新たに定義するhが、重みの各要素に対してどれぐらい調整するのかを個別に調整する機能を担う。
（過去の重み更新を覚えながら、重みの更新量を決定していく。）

〇RMSProp
RMSPropは、AdaGradに類似している（改良版）。hを求めるときに、
右辺の各項の合計の係数が1となるようにαと1-αをつけて割合を決定

RMSPropのメリット
・局所最適解にはならず、大域的最適解となる。
・ハイパーパラメータの調整が必要な場合が少ない。（⇔AdaGrad）

〇Adam
・モメンタムの過去の勾配の指数関数的減衰平均
・RMSPropの過去の勾配の2乗の指数関数的減衰平均
上記のメリットをそれぞれ孕んだ最適化アルゴリズム。

optimizerごとのふるまいの確認をすると、スムーズに収束しているAdamが優れているとわかる。
他のoptimizerだとFtrlも収束しているが、発散しながら収束しているので、評価が困難。