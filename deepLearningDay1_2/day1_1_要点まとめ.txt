入力層から中間層を含む全体像について
・機械（深層）学習モデルを入力・出力の目的で分類すると識別モデル（backward）と生成モデル(forward)がある。
・識別モデルの場合は、あるデータが与えられた条件下でのクラスCk（犬、猫）である確率が、
生成モデルの場合は、あるクラスに属するという条件下でのデータCkの分布が計算結果になる。
・識別モデルの具体例⇒決定木、ロジスティック回帰、SVM、ニューラルネットワーク
・生成モデルの具体例⇒隠れマルコフモデル、VAE、GAN
・特徴としては、識別モデルは高次元⇒低次元（画像認識）、
生成モデルは高次元⇒低次元（画像化）

・2007年頃のまとめ方として、識別器（Classifier）の開発アプローチの分類としては下記3つのアプローチが存在する。
⇒生成モデル、識別モデル、識別関数。
・生成モデルでも、識別ができる（ベイズの定理）、また識別モデルは前述と大体同じ、
識別関数は確率が計算されずに決定的な選別を行う点が大きく異なる。その分学習量が少なくて済む。
・生成モデルはデータの中身もしっかり見て学習（クラス条件付き密度）、識別モデルはそこまでしっかり見ないで学習する（確率を求めるまで）。
・識別モデルは、事後確率を推論してから、識別結果を決定する。そのため、人間が推論結果の取り扱いを判断できる。
・識別関数は、一気に識別結果を決定する。
・「万能近似定理」とは、ニューラルネットワークのような活性化関数をもつネットワークを使うことで、どんな関数でも近似できるのではないか。
・NNは大きく３階層（入力層、中間層、出力層）に分かれている。
・学習で、w(重み),b(バイアス)を決めていき、出力層の各クラスである確率を求める。
・NNは入力から中間層を経た出力結果と実際の結果とを比較して、誤差を取り、フィードバックして再学習する。
・NNが対象とする問題は、回帰と分類がある。
⇒分類は、何種類かに分類。回帰は連続的な繋がりをもったデータの予想をする。ランキングの予想もある。
回帰：連続する実数値を取る関数の近似（線形回帰、回帰木、ランダムフォレスト、NN）
分類：離散的な結果を予想するための分析（ベイズ分類、ロジスティック回帰、決定木、ランダムフォレスト、NN）
⇒NNは、両方に対応する。「万能近似定理」
・４つ以上中間層を持つNNのことを深層NNモデルいう。
・深層学習の実用例として
⇒トレード、チャットボット、翻訳、音声（空気振動）解釈、囲碁、将棋AI

入力層から中間層について
・入力(x)が入力層に、重み(w)とバイアス(b)とから、総入力がu=wx+bで行列表現される。
・w（傾き）とb（バイアス、切片）を学習させていくことを行いたい。
・総入力に対して活性化関数を通すと、出力（次の層の入力）が得られる。
