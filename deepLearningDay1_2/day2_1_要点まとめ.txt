勾配消失問題
・誤差逆伝搬法によるNNを学習する方法がうまくいかないことがある問題。
・中間層が増えると微分の連鎖率の計算が増える。
・誤差逆伝搬法が下位層に進んでいくにつれて、勾配がどんどん穏やかになっていく。
そのため、勾配降下法による更新では、下位層のパラメータはほとんど変わらず、
訓練は最適値に収束しなくなる。
・理由としては、微分した値が0〜1を取る関数だと、その値をかけることで、どんどん0に近づくために消失してしまう。
（例）シグモイド関数--微分すると最高でも0.25しかとらない
→解決方法としては、活性化関数の選択、重みの初期値の設定（選択）、バッチ正規化

〇活性化関数の選択について
・ReLU関数を選択すると、xが0より大きいとき、微分結果が1、それ以外の時、微分結果が0となる。
そのため、勾配消失問題の回避（微分結果が1のとき）とスパース化（微分結果が0のとき）に貢献することで良い成果をもたらす。

〇重みの初期値設定について
・NNでは、乱数（標準正規分布に従った）を用いて、重みの初期値を設定することが多かった。
が、勾配消失問題が起きるため　Xavierの初期値（重みの要素を、前のレイヤーのノード数の平方根で除算した値）を設定する。
以下、補足
・標準正規分布で重みを初期化したときは、各レイヤーの出力は0と1に偏る。
→勾配消失が起き、誤差逆伝搬にはむかない！
・重みの初期値の標準偏差を小さくする（例えば、0.01をかける）と、今度は真ん中付近による
→分散していないので、NNの効果がない。
Xavier初期化だと、活性化関数（S次カーブ型の場合）の表現力を保ったまま、勾配消失への対応がとれる。
・S次カーブ型の関数以外（例えば、Relu関数）の場合の対策としては、He初期値を設定する。
He初期化は、重みの要素を、前の層のノード数の平方根で除算した値に対しルート2をかけ合わせた値。（つまり、ルートの2/nで除算した値）

〇バッチ正規化について
ミニバッチ単位で、入力値のデータの偏りを抑制する手法。
学習量データが多いと、それを細かく分割するが、その分割したものをミニバッチという。
機材（CPU,GPU,TPU）によってミニバッチのサイズは異なる。（例：画像データ1〜64枚)

・バッチ正規化の使いどころとは？
活性化関数に値を渡す前後に、バッチ正規化の処理を孕んだ層を加える。

・バッチ正規化の数学的記述について
ミニバッチの平均、ミニバッチの分散、ミニバッチの正規化、調整動