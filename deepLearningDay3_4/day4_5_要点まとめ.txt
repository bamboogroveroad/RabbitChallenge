●Tranformer
Self-Attention(自己注意機構)に焦点を当てる

・ニューラル機械翻訳の問題点
長さに弱い。翻訳元の内容を一つのベクトルで表現するため、
文章が長くなると表現力が足りなくなる。

上記問題意識から生まれたのが
〇Attention(注意機構)
翻訳先の各単語を選択する際に、翻訳元の文中の各単語の隠れ状態を利用して、
重み(全て足すと1,　重みは意味的に注意に相当)を分配する機構。

つまるところ何をしているのか。
→Attentionは、辞書オブジェクト。query,key,valueの三要素に分けられる。
query(検索クエリ)に一致するKeyを索引し、対応するvalueを取り出す操作であると見做す事ができる。
これは「辞書オブジェクト」の機能と同じである。(KeyValueAttentionと呼ばれる。)

→ニューラル機械翻訳の問題点が改善された。

〇Transformer
Attention is all you need
従来、Encoder-Decoderモデルの基本だったRNNモデルを全く使用しない。そのため計算量が少ない。


●Transformer 主要モジュール

・RNNを使用していないので文字の位置情報を保存できない
 ⇒ Position Encodingで単語ベクトルに単語の位置を付加

・大きくEncoderとDecoderに分かれている。（どちらもSelf-Attentionを使用）
・Self-Attentionのイメージとしては、CNNが１番近い。
→CNNはあるピクセルを中心にした時に、その周囲の周辺情報をConvolutionして抽象化した情報が出て来る。
Attentionもそれに近い事をしている。Self-AttentionとCNNの対応関係はよく言われており、数式は違うが、どちらも文脈依存のConvolution。
2つの違いは、CNNはあくまでウィンドウサイズがあるので、決められた範囲のConvolutionしか行わないが、
Self-AttentionはInputされた全ての情報からそれぞれを定義していくので、ウィンドウサイズが文の長さのConvolutionとも考えられる。

・Encoder
系列をインプットして、その系列が位置情報を失わないままSelf-Attention層を流れて行って、
内部状態に変換される

・Position-Wise Feed-Forward Networks
全結合層（位置情報を保持したまま順伝播させる）。
目的としては、線形変換をかける層。最終的にアウトプットの次元を揃える必要があるのでマトリックスを整えるための全結合層。

・Scaled dot product Attention
全単語に関するAttentionをまとめて計算する。

・Multi-Head Attention
Scaled dot product Attentionを連結したもの。
-8つのScaled dot product、Attentionの出力をConcat,それぞれのヘッドが異なる種類の情報を収集
（8つにunitを分けている意図は、Attentionメカニズムを分離させる事で、それぞれのAttention層が、
それぞれの独自の注意のかけ方を学習していって、総合的により良い注意のかけ方になるのが狙い。
実際、内部のAttentionを可視化してみても、それぞれのAttention層ごとに全然違うAttentionのかけ方をしていて、役割を分担させるアンサンブル学習のような働き）

・Decoder
Encoderと同じく6層
-各層で2種類の注意機構
-注意機構の仕組みはEncoderとほぼ同じ
→Encoder側と同様なMulti-Head Attention。
（Self-Attention(図の下部)とSorce-Target Attention(図の上部)が使用されている。）

・Position Encoding
RNNを用いないので単語別の語順情報を追加する必要がある