●実際のRNNモデルの手法（LSTM）について
・RNNの課題として、時系列を遡るにつれて勾配消失していく。
（、前回の授業で触れた勾配消失の解決方法とは別の方法で、）NW構造自体を変えて解決したものがLSTM。
・勾配消失問題のビジョンについて
（微分した値が0以上1未満だと、微分の連鎖率によって、どんどん0に近づく）

・勾配爆発は、勾配消失の逆の話。勾配が、層を逆伝播するごとに指数関数的に大きくなっていくこと。
⇒活性化関数に恒等関数を利用したりするとおこる。（学習率の推奨されている値があるので、推奨値を使用するとよい。）
・勾配爆発を防ぐための勾配クリッピング手法について

・LSTMモデルで最も重要なのはCEC（Constant Error Carousel（定誤差カルーセル、CEC））、
それを取り囲むようにいろいろな機能がある。

・CESは記憶機能だけをもつ。考えることはしない。（⇔中間層）
・勾配消失及び爆発の解決方法として、勾配(δt-z-1　=　δt-z{Wf'(Ut-z-1)})が1であれば解決できる。

・CECの課題は、入力データについて、時間依存度に関係なく重みが一律である。
⇔NNの学習特性が無いということ。

・何を覚えるのか、覚えたものをどう使うのかを周りがフォローする必要がる。
※前者を入力ゲート、後者を出力ゲートが担当して、課題解決を図る。

・CECに覚えさせる方法を身につけるのかを学習するのが、入力ゲートの働き。
・CECの情報をどのように使うのかを学習するのが出力ゲート
・入力ゲートは、Wi（今回の入力についての重み）、Ui（前回の出力についての重み）とから、CECにどのくらい覚えさせるかを決める。
・出力ゲートは、Wo（今回の入力についての重み）、Uo（前回の出力についての重み）とから、CECが記憶したものをどう使うのかを決める。

●LSTMブロックの課題
・CECは過去の情報がすべて保管されている。過去の情報がいらなくなった場合、削除することができず蓄積される。
→解決策として、過去の情報がいらなくなった場合、そのタイミングで情報を忘却する機能が必要。（忘却ゲートが必要）
・忘却ゲートは、Wf（今回の入力についての重み）、Uf（前回の出力についての重み）とから、忘却するか残すか判断する。
※CECの覚えている情報は、c(t)=i(t)*a(t)+f(t)*c(t-1)で決定される。

・課題として、CECの保存されている過去の情報を、任意のタイミングで他のノードに伝播させたりあるいは任意のタイミングで
忘却させたいというのがある。　CEC自身の値は、ゲート制御に影響を与えていない。

・覗き穴結合（CECの状態を他のゲートが覗くイメージから）は、CEC自身の値に重み行列を介して伝播可能にした構造。
→あまり改善効果がなし