●機械学習のアルゴリズムは教師あり学習、教師なし学習、強化学習に分類。
（教師あり学習、教師なし学習は、データの特徴を見つけるという学習 ⇔ 強化学習は、目標を準備して目標に向かって優れた方策を見つけるという学習する）

・長期的に報酬を最大化できるように環境の中で行動を選択できるエージェントを作る事を目標とする機械学習の一分野
⇒ 行動の結果として与えられる利益（報酬）をもとに、行動を決定する原理を改善していく仕組みです。

・強化学習イメージ図
---------------------------------------------------------------
環境　　　　　　　　　　　　　　状態S　　　　　　　

               　↑　　　　　　　↓   　　　　↓　　

エージェント　方策TT                       　価値V　
　
矢印の説明　　行動　　　　　　　観測　　　　 報酬
---------------------------------------------------------------

●探索と利用のトレードオフ
不完全な知識を元に行動しながらデータを収集。最適な行動を見つけていく。

※過去のデータに基づく行動のみだと探索が足りない　⇔　未知の行動のみだと経験が生かせず、利用が足りない

・強化学習が学習したい場所は2か所（方策、価値）
方策関数:TT(s,a)
行動価値関数:Q(s,a)の２つを学習させる。


●強化学習の歴史について
・冬の時代があったが、計算速度の進展により大規模な状態を持つ場合が可能となりつつある。
・関数近似法（価値関数や方策関数を関数近似する手法）とＱ学習（行動価値関数を、行動する毎に更新する事により学習を進める方法）を組み合わせる手法の登場

●価値関数について　状態価値関数と行動価値関数の２種類がある。
ある状態の価値に注目する場合は、状態（価値）関数
V(s)

状態と価値を組み合わせた価値に注目する場合は、行動価値関数、こっちがよく使われる。
Q(s,a)

⇒ゴールまで今の方策をつづけた時の報酬の予測値が得られる。

●方策関数について
方策ベースでの強化学習手法において、ある状態でどのような行動を採るのかの確率を与える関数のこと。
π(s,a | θ)： VやQを基にどういう行動をとるか。⇒その瞬間の行動をどうするか。

⇒エージェントが採る瞬間の行動を決定。

●方策関数を学習させる方法について
強化学習は価値関数と方策関数の２つが優秀ならうまくいく。
⇒将来のことを考えながら、今の行動を選べる人

・方策勾配法
方策をモデル化して最適化する手法
（★関数はNNにできる、学習できる、方策関数πをNNとして学習させよう！）

方策関数の重みについて
θ(t+1) = θ(t) + εΔJ(θ)
（ｔ：時刻、ε：学習率、J：期待収益（⇔NNでは誤差関数）期待収益は大きくしたい！）

※Jは方策の良さ 

・定義方法
平均報酬、割引報酬和に対応して、行動価値関数の定義を行い、方策勾配定理が成りたつ。
