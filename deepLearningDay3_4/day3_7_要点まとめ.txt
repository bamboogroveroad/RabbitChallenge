・seq2seqの問題は長い文章への対応が難しい。
2単語でも、100単語でも固定次元ベクトルの中に入力しなければならない。
（文章が短いと非常に無駄が多い）

・解決策としては、文章が長くなるほどそのシーケンスの内部表現の次元も大きくなっていく、
仕組みが必要になる。

→Attention Mechanism

⇒「入力と出力のどの単語が関連しているのか」の関連度を学習する仕組み。例は下記

I have a pen.

I⇔私
have⇔持って
a⇔なし（重度度が低く関連なし）
pen⇔ペン　

と関連度を学習する。
