・RNNの応用例、Seq2Seqは、2つのネットワークがドッキングして作られている。
自然言語処理（機械対話、機械翻訳）に用いられている手法。

・モデルの全体図について、一つ目のネットワーク（RNN、LSTM、GRU等）には単語の列を順々に入力していくと、
中間層には、単語がたまっていくので、文脈となり保存、意味がベクトル表現として保持される。

・保持された文脈の情報を、次のネットワークに渡す。次のネットワークは別の表現を得る。（英語を日本語に翻訳するイメージ）
→1つ目のネットワークをEncoder,2つ目のネットワークをDecoderという。

⇒Seq2Seqは、Encoder-Decoderモデルの1種。

〇Encoder RNN・・・　文の意味を集約する。(文脈の意味ベクトルを抽出する)
具体的には（※自然言語処理の場合は、数万語について行う。）下記を行う。

●Taking：文章を単語等のトークン毎に分割、トークン毎のIDに分割する（ONE HOT VECTOR）。

●Embedding：IDからそのトークンを表す分散表現ベクトル（数百位の大きさに抑えられる。方法は、※下記のように機械学習で変換）に変換。
※単語の意味が似ているものは、数字の並びが似かよるように学習させる ⇒ 単語の意味を抽出したベクトル。（特徴量抽出）

●Encoder RNN：ベクトルを順番にRNNに入力していく。手順は下記。

1.vec1をRNNに入力し、hidden stateを出力。
このhidden stateと次の入力vec2をまたRNNに入力してできたhidden stateを出力という流れを繰り返す。

2.最後のvecを入れた時のhidden stateをfinal stateとしてとっておく。このfinal stateがthought vectorと呼ばれ、
入力した文の意味を表す（文脈を表す）ベクトルとなる。

・特徴量抽出が難しい。
自然言語ではBERT（Google）が優秀。

★BERTの事前学習に用いられる手法、MLM - Masked Language Modelについて、
「私は昨日ラーメンを食べました。」

→上記文の「昨日」のところを隠して、前後の文脈から「昨日」を予測できるように
文の意味ベクトルを学習させる。

〇Decoder RNN・・・RNNが出力してきた隠れ層の情報を基に、別の文脈を生成する。手順は下記

1.Encoder RNNのfinal state(thought vector)から、各tokenの生存確率を出力していきます
final stateをDecoder RNNのinitial state として設定し、Embeddingを入力

2.Sampling 生成確率にもとづいてtokenをランダムに選びます。

3.Embedding 2.で選ばれたtokenをEmbeddingしてDecoder RNNへの次の入力とします。

4.Detokenize 1.〜3.を繰り返し、2.で得られたtokenを文字列に直します。
（Encoder RNNの逆の動き、ベクトル表現から単語を戻す。）


・Seq2Seqの課題　一問一答しかできない。
→問いに対して文脈も何もなく、ただ応答が行われ続ける
→改善したのがHRED

・HREDは、文の意味（文脈）ベクトルを引き継ぐことで、過去n-1個の発話から次の発話を生成する。
→Seq2Seqでは、会話の文脈無視で応答がなされたが、HREDでは、前の単語の流れに即して応答されるため
より人間らしい文章が生成

HREDの課題：
HREDは確率的な多様性が字面にしかなく、会話の流れのような多様性がない
→同じコンテキスト（発話リスト）を与えられても、答えの内容が毎回会話の流れとしては同じものしかだせない。

HERDは短く情報量に乏しい答えをしがちである。
→短いよくある答えを学ぶ傾向がある。
ex) うん、そうだね、・・・

→改善したのがVHRED
HREDに、VAEの潜在変数の概念を追加したもの。
→HREDの課題を、VAEの潜在変数の概念を追加することで解決した構造。

・オートエンコーダ
教師なし学習の1つ。学習時の入力データは訓練データのみ。
具体例としてはMINSTの場合、28*28の数字の画像を入れて、
同じ画像を出力するNN。
具体的には下記
（→をEncoderが、⇒をDecoderが行う。）
入力データ28*28（画像）→　潜在変数：z（ずっと小さい。100ぐらい）⇒　　出力データ28*28（入力と同じ画像）

・VAE
通常のオートエンコーダーの場合、何かしら潜在変数：zにデータを押し込めているものの、その構造が不明
→VAEは潜在変数：zに確率分布z〜N(0,1)を仮定したもの。

→VAEはデータを潜在変数：zの確率分布という構造に押し込めることを可能にする！
⇒元のデータの近さ、遠さもある程度保たれることになる。

●実際にVAEを学習するときは、
Endoderの出力にノイズを加えて（ドロップアウト）、ノイズのついた潜在変数：zからDecoderは元の画像に戻ことで
汎用性が高まる。