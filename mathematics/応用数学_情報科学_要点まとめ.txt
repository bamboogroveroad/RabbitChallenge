・増えた量が同じ場合、増加の比率が違う（大きい）ほうが変化がとらえやすい。
・人間は、情報のわかりやすさを比率でとらえている。
・1/wを積分したら、logWになるがこれを自己情報量というが、
wと確率は逆数になる（wは事象なので）ので、Pの関数にして負の符号をつけて表す。
・対数の底が２の時は、ビット、ネイピアの時は、ナットが単位になる。
・例えばスイッチは掛け算でバリエーションの数がわかるのでlogをとってあげると
必要な数がわかるのでそのように考えてもよい。
・自己情報量の期待値をのことをシャノンエントロピという。
・コイン投げのときのシャノンエントロピの例について。
・シャノンエントロピは誤差関数の中身としても利用できる。
・カルバック・ライブラー　ダイバージェンスとは
同じ事象・確率変数における異なる確率分布P、Qの違いを表す。
・P、Qの性質の違いを表していて、自己情報量の引き算を元に
P（新たに分かった分布）の期待値で算出できる。
・KLダイバージェンスは誤差関数の中身としても利用できる。
・交差エントロピーは、KLダイバージェンスの交差部分を取り出したもの。