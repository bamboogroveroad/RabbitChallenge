・線形回帰モデルについて。線形とはざっくり比例関係のこと。
・4次元以上の空間における超平面の方程式についてをベクトルと転置をつかった方程式で表現する。
・回帰問題は、ある入力から出力を予測する問題。
・回帰問題は予測だが、ランキング問題にも利用できる。
・ただし、バプニックの原理の考え方等もあるので注意。
・回帰で扱うデータについて、入力はm次元のベクトル（m=1の場合はスカラ）、出力はスカラー値（目的変数）。
・線形回帰モデルは、教師あり学習、入力とm次元パラメータの線形結合を出力するモデル。
（※慣例として予測値にはハットをつける（推定値であって正解データとは異なる。））
・線形結合は入力xと未知のパラメータwの内積に切片w0をたした形。
・パラメータは最小二乗法で求める。
・線形回帰モデルのモデル数式について。（単回帰モデルの場合）。
・誤差についてもいろいろな種類（偶発誤差以外）があるので注意する。
・線形回帰モデルの連立方程式と行列表現について（単回帰モデルの場合）。
・yのi番目の表現と、それを基にした全体での行列表現について。
・線形回帰モデルのモデル数式について。（重回帰モデルの場合）。
・線形回帰モデルの連立方程式と行列表現について（重回帰モデルの場合）。
・係数の行列のことを計画行列という。n×（m+1）行列の形。
・n（データ数）がm+1（未知のパラメタ数）より大きくないと解くことは厳しい。
・データは、学習用データ（パラメタの決定用）、検証用データに分割して利用する。
・未知のデータにも予測できるように汎化性能を高める必要がある。
・パラメータは最小二乗誤差で推定する。学習データの平均二乗誤差を最小にするパラメータは何か。
・なぜ2乗するのか、差の絶対値や他のやり方はないのか。実は、2乗損失は一般に外れ値に弱いので、Huber損失、Tukey損失のほうが影響を受けにくい。損失関数の取り方はいろいろある。
・MSEが最小となる入力を求めるので、argをつける。
・MSEをwで偏微分して0の時の値を求める。結果として、回帰係数が求まり、予測値が求まる。
・逆行列は常に存在するではない。その場合の一般化逆行列について。
・ボストンの住宅価格についての推定例（skl_regression.ipynb）⇒住宅価格が負になりうまくいかない（外挿問題）。