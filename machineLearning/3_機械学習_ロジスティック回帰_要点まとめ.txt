・ロジスティック回帰モデルは、分類タスク。
・ある入力（数値）からクラスに分類する問題である。
・入力（各要素を特徴量または説明変数と呼ぶ）はm次元のベクトル(m=1の場合はスカラ)、出力（目的変数）は0か1。
分類タスクには下記のアプローチがある。
・識別的アプローチは、P(Ck/x)を直接モデル化する⇒ロジスティック回帰モデルは識別的アプローチ。
・生成的アプローチは、P(Ck)と、P(x/Ck)をモデル化して、その後ベイズの定理によりP(Ck/x)を求める。
・識別関数の構成もある（SVMなど）
・線形結合をsigmoid関数を利用して、実数全体から[0,1]につぶすことができる。
・シグモイド関数の性質として、微分するとシグモイド関数で表せる。
・シグモイド関数の出力を、y=1になる確率に対応させる。
・一番シンプルな方法は、データYは確率が0.5以上ならば1,未満なら0と予測する。
・wをどう求めるか。
・様々な確率分布（ベルヌーイ分布、正規分布、t分布等）があるが、ロジスティック回帰ではベルヌーイ分布を利用する。
・データからそのデータを生成したであろうもっともらしい分布（パラメータ）を推定したい⇒最尤推定、尤度関数を最大化する。
・尤度関数Lを最大化するパラメータを探す（推定）
・但し、確率をかけると非常に小さくなってしまう（0に近づく）ので、
対数をとって、対数尤度関数を考える。また最小化問題にするためにマイナスをつける。
・ただ、最適解をすぐに求めるのは困難なため、徐々に最適解に近づけていく方法である勾配降下法を利用する。
・傾きは、負の対数尤度関数を微分のchainルールを利用して微分して求める。
・勾配降下法では、パラメータを更新するのに全データに対する和を求めるためにメモリ容量が不足する、計算時間がかかる等の課題。
・上記を解決するためにデータをランダムに選んでパラメータを更新する、確率的勾配降下法（SGD）で更新すると良い。
・性能を測る指標として、モデルの予測結果と検証用データの結果のマトリックス（混同行列）について。