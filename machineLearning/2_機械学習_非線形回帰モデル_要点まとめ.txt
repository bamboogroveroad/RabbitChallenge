・非線形な回帰を考えたい。例えば2次、３次の項があるまたは、正弦関数、余弦関数、対数の項がある等。
⇒アイディアとして、線形の時のxの代わりにΦ(x)を用いる。
・xからΦ(x)に代えても、w（重みパラメータ）については線形のまま。
（linear - in - paprameter）
・以上より、線形モデルによる非線形回帰が正確な表現といえる。
・Φ(x)を基底関数という。
・基底関数Φ(x)に多項式関数Φj=Xのj乗を考える。
w1からw9にxの１乗から9乗がそれぞれかけられて合計される。
・ガウス型基底関数について
・基底展開法も線形回帰と同じ枠組みで推定可能
・先ほどの、xだったところがΦ関数で飛ばされるだけで後は同じになる。
以下、汎化の性能を上げる方法について
・多項式でも5次以降だと無駄が多い（オッカムの剃刀）
・学習データに対して、十分小さな誤差が得られないモデル⇒未学習が起きる。
対応は？
⇒次元を上げて対応する。
・小さな誤差は得られたけど、テスト集合誤差との差が大きいモデル⇒過学習が起きる。
対応は？
⇒学習データ数を増やす対応
⇒不要な基底関数（変数）を削除して表現力を抑止する対応（特徴量の選択、AICによる情報選択）
⇒正則化法を利用して表現力を抑止する対応
・正則化法については、MSEに、Wが大きくなると大きくなる関係のある罰則項を
加えたものを小さくするイメージ。
解きたいのは、MSEの最小化、ただし罰則項R(W)がrより小さい条件を満たす必要あり。
⇒KKT条件より、min　MSE + λR(W)　を考えればよい
・正則化項（罰則項）、MSE、Ridge、Lasso推定量について
・正則化パラメータλの求め方について
・ホールドアウト法は、有限のデータを学習用とテスト用の2つに分割し、「予測精度」や「誤り率」を推定するために使用。
⇒手元にデータが大量にある場合を除いて、良い性能評価を与えないという欠点あり。
⇒欠点を補うためにクロスバリデーション（CV：交差検証）を行う。学習用と評価用のデータを順番に変えていくとよい。
・グリッドサーチについては、すべてのチューニングパラメータの組み合わせで評価値を算出して、もっとも良い評価値をもつ
組み合わせを採用する方法をいう。組み合わせ爆発回避のため、現実的にはベイズ最適化等が利用される
